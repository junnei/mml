---
layout: default
title: 확률적 모델링 및 추론
lang: kr
lang-ref: probabilistic-modeling-and-inference
parent: When Models Meet Data
permalink: /kr/when-models-meet-data/8-4
nav_order: 4
writer: jj150618
---

# 확률적 모델링 및 추론
{: .no_toc }


Chapter 4 : Probabilistic Modeling and Inference
{: .fs-5 .fw-300 }


{% include writer.html writer=page.writer lang=page.lang %}

---

- 목차
    {: .text-gamma }

    1. TOC
    {:toc}

---

## 확률적 모델링

확률적 모델링에서 관측 변수 $x$와
숨겨진 파라미터 $\theta$의 
결합 분포(joint distribution) $p(x,\theta)$가 제일 중요한 부분이다.
이 결합 분포는 아래의 내용을 내포하고 있다.

- The prior and the likelihood (사전 확률과 우도)
- The marginal likelihood $p(x)$
- The posterior (사후 확률)

결합 분포만이 이런 특성을 가지고 있기 때문에 확률적 모델링은 모든 확률 변수의 결합 분포로 명시할 수 있다.

## 베이지안 추론

머신러닝의 목적은 모델과 데이터을 구하고 관측 변수 $x$에
따른 모델의 숨겨진 파라미터 $\theta$ 값을 알아내는 것이다.
우리는 앞에서 MAP와 MLE를 통해서 모델의 파라미터를 추정하는 것을 배웠다.
최적화 문제를 해결할 수 있는 $\theta$
에 대한 
single-best value인 
${\theta}^{*}$

를 얻어서 이를 예측에 사용할 수 있다.
이 값을 이용한 예측 분포는 
$p(x|\theta^{*})$

이다.
하지만 이러한 사후 분포에서 일부 통계에만 초점을 맞추는 경우 정보 손실이 발생하고 $p(x|\theta^{*})$

만을 사용해서 결정을 내리면 문제가 발생할 수 있다.
베이지안 추론은 완전한 사후 확률 분포를 찾는 것이다.


주어진 데이터셋 $\mathcal{X}$에 대해 베이즈 정리를 통해서 아래와 같은 공식을 알 수 있다.

$$
p(\boldsymbol{\theta}|\mathcal{X})=\frac{p(\mathcal{X}|\boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{X})}, \quad p(\mathcal{X})=\int p(\mathcal{X}|\boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$

예측한 파라미터에 대한 분포인 $p(\theta)$를 이용하여 아래와 같은 식을 얻을 수 있다.

$$
p(\boldsymbol{x})=\int p(\boldsymbol{x}|\boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}=\mathbb{E}_{\boldsymbol{\theta}}[p(\boldsymbol{x}|\boldsymbol{\theta})]
$$

MLE나 MAP에서 연산의 핵심 문제는 최적화지만, 베이지안 추론에서는 적분이 연산의 핵심 문제가 된다. 사전분포(Prior distribution)와 사후분포(Posterior distribution)가 동일한 분포족에 속하면 사전분포를 켤레 사전 분포(conjugate prior distribution)라고 하는데, 여기에 해당하지 않으면 앞에 나왔던 식에 대한 적분 계산이 어렵고 다른 값들도 계산할 수가 없다. 이런 경우에는 Markov chain Monte Carlo (MCMC)와 같은 확률적 근사치나 라플라스 근사치나 variational inference, expectation propagation와 같은 결정적 근사치를 사용 할 수 있다.

## 잠재 변수 모델

때때로 잠재 변수(latent variables) $z$를 갖는 것이 유용한 경우가 있다.
이러한 잠재 변수는 모델을 명시적으로 파라미터화 하지 않기 때문에 파라미터 $\theta$와 다르고 모델의 구조를 단순화하고 데이터 생성 프로세스를 설명할 수 있게 한다.

$p(x|\theta,z)$는 
데이터 $x$와 모델 파라미터
$\theta$, 그리고 잠재 변수
$z$를 통해 구한 조건부 분포이다.

앞에서 나왔던 공식을 이용하여 잠재 변수 $z$에 대한 사후 확률을 구해보자.

먼저, 잠재 변수에 의존하지 않는 모델의 우도 
$p(x|\theta)$를 계산한다.


$$
p(\boldsymbol{x}|\theta)=\int p(\boldsymbol{x}|\theta,\boldsymbol{z}) p(\boldsymbol{z}) \mathrm{d} \boldsymbol{z}
$$

그리고 아래 공식을 토대로 
$p(\boldsymbol{\theta}|\mathcal{X})$를 구할 수 있다.

$$
p(\boldsymbol{\theta}|\mathcal{X})=\frac{p(\mathcal{X}|\boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{X})}, \quad p(\mathcal{X})=\int p(\mathcal{X}|\boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$

그리고 아래 공식을 토대로
$p(\boldsymbol{z}|\mathcal{X})$와
$p(\mathcal{X}|\boldsymbol{z})$를 구할 수 있다.


$$
p(\boldsymbol{z}|\mathcal{X})=\frac{p(\mathcal{X}|\boldsymbol{z}) p(\boldsymbol{z})}{p(\mathcal{X})}, \quad p(\mathcal{X}|\boldsymbol{z})=\int p(\mathcal{X}|\boldsymbol{z}, \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$

그래서 최종적으로 우리는 
$p(z|\mathcal{X},\theta)$를 구할 수 있다.

$$
p(\boldsymbol{z}|\mathcal{X}, \boldsymbol{\theta})=\frac{p(\mathcal{X}|\boldsymbol{z}, \boldsymbol{\theta}) p(\boldsymbol{z})}{p(\mathcal{X}|\boldsymbol{\theta})}
$$

---

{% include category.html category=page.parent id=4 %}
