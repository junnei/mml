---
layout: default
title: 제약 최적화와 라그랑주 승수법
lang: kr
lang-ref: 7-2
parent: 연속 최적화
permalink: /kr/continuous-optimization/7-2
nav_order: 2
writer: sulogc
---

# 제약 최적화와 라그랑주 승수법
{: .no_toc }


Chapter 2 : Constrained Optimization and Lagrange Multipliers 
{: .fs-5 .fw-300 }


{% include writer.html writer=page.writer lang=page.lang %}

---

- 목차
    {: .text-gamma }

    1. TOC
    {:toc}

---

이전 7.1 장에서 $ f  :  \mathbb{R}^d \rightarrow $ 에서 정의되는 함수의 최소값을 찾는 문제를 보았다. 이번 장에서는 제약을 추가할 것이다. $ i = 1, ..., m $ 에 대해, $ g_i  :  \mathbb{R}^d \rightarrow $ 일 때, 

<br>

$$ 
\begin{matrix}
\min\limits_{x} & f(x) & \\

\text {subject to  } & g_i(x) \leqslant 0 & \text {   for all i = 1, ..., m }

\end{matrix}
$$ 

<br>

함수 $ f \text { 와 } g_i $ 가 일반적인 경우 볼록 함수가 아닐 수 있다. 볼록 함수인 경우는 7.3 장에서 다루겠다. 제약 문제를 비제약 문제로 바꾸는 명확한 방법은 지시함수를 사용하는 것이다. 다음은 지시함수 infinite step function이다. 

<br>

$$ 1(z) = 
\begin{cases}
0 & \mbox{if } z \leqslant 0  \\
\infty & \mbox{otherwise}
\end{cases}
$$ 

<br>
이를 활용하여, 다음과 같은 새로운 함수의 최소값을 구하는 문제로 바꿀 수 있다.

<br>

$$
\begin{align}
     J(x) = f(x) + \sum_{i=1}^m {1(g_i(x))}
\end{align}
$$

<br>

제약으로 0보다 작아야하는 $ g $ 가 0 이상일 경우 $ J(x) $가 발산해 버린다. 물론 infinite step function 또한 최적화 하기가 까다롭다. 여기서 라그랑주 승수법을 소개한다.

라그랑주 승수법은 위와 같이 제약식을 목적함수에 표현하기 위한 step function을 선형함수로 대체할 수 있다. 


---

{% include category.html category=page.parent id=2 %}
