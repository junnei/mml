---
layout: default
title: 역전파와 자동 미분
lang: kr
lang-ref: backpropagation-and-automatic-differentiation
parent: Vector Calculus
permalink: /kr/vector-calculus/5-6
nav_order: 6
writer: bluvory
---

# 5.6 Backpropagation and Automatic Differentiation
{: .no_toc }


Chapter 6 : Basis and Rank
{: .fs-5 .fw-300 }


{% include writer.html writer=page.writer lang=page.lang %}

---

- 목차
    {: .text-gamma }

    1. TOC
    {:toc}

---

# 5.6 Backpropagation and Automatic Differentiation

수많은 머신러닝 어플리케이션에서는 gradient descent를 실행하기 위해 좋은 모델의 파라미터를 찾는 것이 중요하다. 이는 모델의 파라미터에 대한 학습 목표의 gradient를 연산한ㄴ 데에 큰 영향을 미친다

우리는 section 5.2.2 에서 주어진 목적 함수에서 calculus를 통해 모델의 파라미터에 대한 gradient를 구하는 것을 배웠고, section 5.3에서는 linear regression의 파라미터에 대한 squared loss의 gradient를 구하는 것에 대해 간략하게 다루었다

하지만 이는 정확하지만 실용적인 방법은 아니다
deep neural network를 훈련할 때는 backpropagation algorithm을 사용한다
이는 모델 파라미터에 대한 error function의 gradient를 연산할 때보다 효율적이다


## 5.6.1 gradient in a deep network
deep learning에서 다단계 함수로 구성된 function value y는 아래와 같은 방식으로 연산된다
여기서 x는 input, y는 실제값, 모든 function은 고유의 파라미터를 가지고 있다
여러층의 neural network에서 각 층에 activation function을 가지고 있다

## 5.6.2 Automatic differentiation

---

{% include category.html category=page.parent id=6 %}
