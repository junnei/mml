---
layout: default
title: 확률 공간
lang: kr
lang-ref: Construction-of-a-Probability-Space
parent: 확률 분포
permalink: /kr/probability-and-distributions/6-1
nav_order: 1
writer: Kim-Ju-won
---

# 확률 공간
{: .no_toc }


Chapter 1 : Construction of a Probability Space
{: .fs-5 .fw-300 }


{% include writer.html writer=page.writer lang=page.lang %}

---

- 목차
    {: .text-gamma }

    1. TOC
    {:toc}

---
## 확률의 목적
확률에 관한 이론적 목적은 실험의 결과 값들을 설명하기 위해 수학적 구조를 정의하는 것이다.

>예로 하나의 코인을 던질 때, 단 한번의 결과를 예측할 수는 없지만 무수히 많은 횟수의 코인을 던진다면 평균적인 결과 값의 규칙성(regularity)를 발견할 수 있다. 

이러한 수학적인 구조(mathematic structure)를 이용하여 **확률의 최종적인 목적은 자동화된 추론(automated resoning)을 하는데 있으며**, 이러한 관점에서 확률은 논리적 추론을 일반화한다.(Jaynes 2003)

## 확률의 철학적 문제
자동화된 추론(automated resoning) 시스템을 구축할 때 전통적으로 사용했던 부울 논리(Boolean Logic)는 플로저블 추론(plausible logic) 형태의 타당성을 제공하지 못한다. 다음의 예시를 살펴보자 

> 예시)  
- A,B라는 명제가 있다고 가정하자. A라는 명제가 '거짓'임이 밝혀질 경우, 전통적인 방식의 부울 논리는 B에 대한 어떠한 논리도 제공해주지 않는다. 하지만 일상적인 생활 속에서 해당정보는  B가 덜 그럴듯하다고 판단하게 만들어줄 수 있또한, 다. B라는 명제가 '참'이라고 밝혀질 경우, B가 더 그럴듯해진다고 판단할 수 있다. 우리는 이러한 방식의 논리를 매일 반복적으로 사용한다. 아래의 예시는 조금더 구체적인 상황을 제공한다.
  - $H1$ : 친구가 제시간에 온다.
  - $H2$ : 친구가 교통체증으로 인해 늦는다.
  - $H3$ : 친구가 외계인에게 잡혔다. 
  이 상황에서 만일 친구가 늦게 오는 것으로 관찰 될 경우 제시간에 온다라는 가설은 H2의 가설이 거짓임에 따라 H1의 가설은 기각된다. H3와 H2의 가설 중 H3는 일반적인 경우에 발생하지 않음으로 H2가 가장 그럴듯한 가설 및 추측이 될 수 있다. 즉, H1, H2, H3의 세가지 가설은 서로 다른 가설이지만 서로의 '참','거짓'에 영향을 줌을 알 수 있다.
  
이러한 방식으로 본다면, 확률 이론은 부울 논리의 일반화(Generalization of Boolean Logic)로 여겨질 수 있다. 머신러닝 관점에서, 자동 추론 시스템의 설계를 공식화하기 위해 종종 이러한 방식을 사용하곤 한다. 확률 이론이 어떻게 추론 시스템의 토대가 될 수 있는가에 대한 추가적인 논거는 Pearl (1988)에서 찾을 수 있다.

꾸준하게 확률의 철학적 기반은 연구되어 왔고, E.T. Jaynes(1922–1998)는 다음과 같이 확률이 모두 만족해야하는 세가지 수학적 조건을 제시하였다.

  1. 신뢰성의 수준은 반드시 실수로 정의된다.
  2. 실수는 반드시 통상적인 관념에 기반을 두어야한다.
  3. 추론의 결과는 다음 세가지를 만족하는 일관성(consistency)를 가져야한다.
  - (a) **일관성과 모순없음(consistency and non-contradiction)**: 모든 경우에 대해 동일한 신뢰성 값을 가져야 한다.
  - (b)**정직성(honesty)**: 모든 데이터를 고려한 결과여야한다.
  - (c) **재현성(Reproduct)**: 두 문제에 대한 같은 지식을 가지고 있다면, 동일한 수준의 신뢰성에 도달해야한다.


<details>
  <summary>참고</summary>
  <p>머신러닝과 기계학습에서는 확률에 대한 두가지의 주요한 해석이 있습니다. 
    <br/>1.<b>베이지안 해석(Bayesiam interpretation)</b>(Bishop, 2006) 
    <br/>베이지안 해석에서는 확률을 사용하여 사건에 대한 불확실성을 정의한다. 이는 "주관적 확률(subjective probability)" 또는 "믿음의 정도(degree of belief)"라고도 불린다. 
    <br/>2.<b>빈도주의 해석(frequent interpretation)(Efron and Hastie, 2016)</b>.<br/>빈도론적 해석에서는 발생한 전체 사건 수에 대한 관심 사건의 상대적 빈도를 고려한다.
  </p>
</details>

몇개의 머신러닝의 확률론적 모델에대해 lazy notation과 전문 용어 같이 사용하여 혼란을 주기도 하며, 이 챕터에서도 같은 방식을 선택했다. 여러개의 개면을 모두 "확률 분포"라고 하며, 스스로 의미를 파악해야 한다. 따라서 머신러닝에서 쓰이는 확률 분포를 이해하는데 한가지 팁을 준다면 범주형(이산형 확률 변수) 또는 연속형(연속 확률 변수) 분포인지 구분하여 이해하는 것이다. 해당 개념은 머신러닝에서 매우 주요하게 쓰이는 개념들이기 때문이다.

## 확률과 확률 변수

확률에 대한 논의에서 앞서 종종 혼동되는 다음 세가지 개념을 확실히 이해하고 있어야한다. 

1. **확률 공간(probability space)** : 확률을 정량화 할 수 있게 하는 개념. 하지만 이 기본(basic)확률 공간을 사용하여 확률을 다루지 않는다.  
2. **확률 변수(random variable)** : 확률 변수로 종종 숫자를 이용하여 확률 공간을 접근하며, 확률을 다룰 때 자주 사용한다.
3. **확률 분포(probaility distribution)**: 확률 변수와 함께 활용하는 개념으으로 6.2챕터에서 다룰 예정이다.

### 현대의 확률
  
현대의 확률은 Kolmogorov(Grinterse and Snell, 1997; Jaynes, 2003)에서 제시하는 아래 세가지 공리에 기반한다. 
  1. **표본 공간(the sample space)**
  2. **사건 공간(the event space)**
  3. **확률 측정(probability measure)**
  
확률 공간은 임의 결과를 갖는 현실 세계(실험)을 모형화한다. 소개한확률공간은 

>**1. 표본 공간(The sample space $\Omega$)**
 표본 공간은 가능한 모든 실험 결과의 집합이며, $\Omega$로 표기한다. 예를 들어, 두 번의 연속적인 동전던지기는 {$ hh, tt, ht, th$}이다.(단, "${h}$"=`앞면`,"${t}$"=`꼬리`)
 <br/><br/>**2. 사건 공간(the event space $A$)**
 사건 공간은 실험의 잠재적 결과의 집합이다. 표본 공간 $\Omega$의 부분집합은 사건 공간 $A$에 포함된다. ($\omega \in \Omega$ 이면 $A$에 속함)따라서 사건 공간 $A$는 표본 공간 $\Omega$의 부분집합을 통해 구할 수 있으며, 이산 확률의 경우 $\Omega$의 멱집합을 통해 구할 수 있다. 
 <br/><br/>**3. 확률 $P$(The probability $P$)**
 ($A \in A$)를 만족하는 모든 사건에대해서, $P(A)$ 는 해당 조건이 만족하는 사건이 발생할 정도 혹은 확률을 의미한다. 그리고 $P(A)$ 는 $A$의 확률이라고 말한다. 

<details>
<summary>참고2. 표본공간(Sample Space)의 다른 이름</summary>
<p>표본 공간(The sample space)은 다른 책에서 다른 이름으로 불리기도 한다. 많이 쓰이는 다른 이름은 "state space"이며 가끔은 동역학계(dynamical system)의 상태를 부르기위해 쓰이기도 한다. 또 다른 이름은 “sample description space”, “possibility space,” and “event space”등이 있다.</p>
</details>

그리고 다음과 같은 조건을 만족해야 한다. 

a. 단일 사건의 확률은 $[0,1]$에 있어야하고 모든 표본 공간의 확률은 1이다. $P(\Omega)=1$로
b. 확률 공간 $(\Omega, A, P)$에서 우리는 실제 현상을 담고 있어야 한다.(실제 현상을 모델링 할 수 있어야 한다. )

### 타겟 공간(Target Space)

머신러닝에서 우리는 확률 공간을 명시하는 것을 피하며 대신에 우리가 관심있는 분야의 확률을 $T$라고 나타낸다. 이 글에서 $T$는 Target Space를 의미하며, $T$의 구성요소를 상태(state)라고 부른다.

### 확률 변수(random variable)

확률 변수는 $\Omega \rightarrow T$를 맵핑해주는 것을 **확률 변수(random variable)라고**  한다. 확률 변수는 $\Omega$의 원소(결과)를 가지고 $T$의 사건의 특정 값 $X$를 반환하는 함수 $X: \Omega \rightarrow T$이다. 

>예시)
동전 두개를 연속적으로 던지는 예에서, 앞면의 갯수를 세는 확률 변수 $X$가 있다고 하면, 다음과 같이 세가지 값을 얻을 수 있다. 
<br/>$$ X(hh) = 2, X(ht) = 1, X(th) = 1,  X (tt) = 0 $$
<br/>그리고 이에 따른 $T$는 다음과 같이 나타낼 수 있다.
<br/> $$ T = \left \{ 1,2,3 \right \} $$

유한한 공간 $\Omega, T$에 대하여 확률변수는 룩업 테이블(lookup table)로 쓰인다. $S \subseteq T$인 모든 사건에 대해 확률 변수 X에 대한 확률을 $P_x(S) \in [0,1]$의 원소로 맵핑시킨다. 다음은 해당 개념에 대한 예시이다. 

>**예시 3)**
가방에서 두개의 동전을 꺼내 확인하는 문제를 생각해보자. 하나의 가방에는 U.S의 동전을 $S$로 표시하고, 유럽의 동전을 $E$로 표기한다고 가정해보자. 이 때 표본 공간 $\Omega$는 총 4가지 경우 $(S,S),(S,E),(E,S),(E,E)$가 가능하다. 그리고 이 때에 $S$를 뽑을 확률을 0.3이라고 가정한다.
<br/>이때 우리가 $S$가 뽑히는 횟수에 관심을 둔다고 하면, 확률 변수 $X: \Omega \rightarrow T$를 정의하여 $S$를 반환하는 횟수를 나타낸다고 하자. 우리는 앞선 경우에서 $T=${$1,2,3$}임을 알 수 있으므로 룩업 테이블을 다음과 같이 표기 해줄 수 있다. 
<br/>$$X((S,S))=2, X((E,S))=1, X((S,E))=1, X((E,E))=0$$<br/>
첫번째 동전과 두번째 동전은 독립적으로 시행되므로(Section 6.4.5) 각 확률변수에 대한 확률질량함수는 다음과 같이 정의된다.(Section 6.2.1)
<br/>$$P(X = 2) = P((S,S)) = P(S) · P(S) = 0.3 · 0.3 = 0.09$$
<br/>$$P(X =1)=P((S,E)∪(E,S)) = P ((S, E)) + P ((E, S))$$
<br/>$$= 0.3 · (1 − 0.3) + (1 − 0.3) · 0.3 = 0.42$$
<br/>$$P(X = 0) = P((E,E)) = P(E) · P(E)
= (1 − 0.3) · (1 − 0.3) = 0.49$$

이 연산에서 $P(X = 0) = P((E,E))$로 한 것 처럼 우리는 표본 공간(The sample space $\Omega$)의 확률과 X의 결과값을 동일시하여 계산을 했다. 두개의 동전을 연속해서 던지는$T$의 하나의 결과값 같이 $X: \Omega \rightarrow T$와 $S \subseteq T$를 만족하는 $T$에 대해서 생각해보자. $X^{−1}(S)$를 $X$의 pre-image라고 라고 가정해보면,  $\Omega$의 원소는 $S$에 $X; {\omega \in \Omega : X(\omega) \in S}$를 이용해 맵핑할 수 있다. 해당 관계는 아래와 같이 나타낼 수 있다. 

$$P_X(S)=P(X \in S) = P(X^-1(S)) = P( \{ \omega \in \Omega : X(\omega) \in S\})$$

좌변을 통해 우리가 관심을 가지고 있는 확률임을 알 수 있으며, 이는 확률변수 $X$를 통해 $\Omega$에 매핑됨을 알 수 있다. 따라서 확률변수(X)는 특정한 확률분포($P_X$)에 매핑되어 있다고 볼 수 있다. 따라서 이때의 $P_X$ 는 law 또는 분포(distribution)라고 부른다.

<details>
<summary>참고3. Target Space의 종류</summary>
<p>여기서 Target Space는 확률 변수 X를 나타내기 위해 쓰였다. T가 유한하거나 무한히 셀수 있으면 이산확률변수(discrete random variable)이라고 부르며, 셀수 없이 무한한 경우 연속확률변수(continuous random variable)이라고 부른다.</p>
</details>

## 통계(Statistics)

통계(Statistics)는 종종 확률분포와 함께 소개되지만, **불확실성의 다른 측면을 다룬다**. 따라서 이 둘은 어떠한 확률 통계적 문제를 접근할 때 다른 방식으로 접근한다.

|확률|통계|
|-|-|
|확률로 접근한 프로세스 모델을 다루거나 확률 변수를 기반으로 하여 어떠한 일이 일어나는지 접근하여 해당 사건에 대한 확률적 규칙을 발견하는데 초점을 둔다. |이미 일어난 일에 대해 다루며, 해당 관측값에 대한 설명을 할 수 있는 프로세스에 초점을 준다.|

머신러닝은 데이터에 대해 설명할 수 있는 최적화된 모델을 만든다는 측면에서 통계와 가깝다고 할 수 있다. 하지만, 여기서 확률을 사용하여 데이터에 최적화(best-fitting)된 값을 찾을 수 있도록 노력한다. 

<details>
<summary>참고4. 머신러닝과 일반화 오류에 대한 추가자료</summary>
<p>머신러닝의 또 다른 측면은 우리가 "일반화 오류"에 관심이 있다는 것이다.( Section.8). 만든 모델의 미래의 실제 데이터가 과거에 데이터에 대한 모델링을 통해 예측값과 다르다는 것을 의미한다. 미래예측은 확률과 통계에 기반하지만, 대부분은 내용은 이 챕터의 범위를 벗어난다. 해당 부분은 Bucheron 외(2013)와 Shalev-Shwartz 및 Ben-David(2014)의 책을 통해 공부할 수 있다. 이외의 통계에대한 추가적인 내용은 Section 8에서 다룰 예정이다</p>
</details>

---

{% include category.html category=page.parent id=1 %}
